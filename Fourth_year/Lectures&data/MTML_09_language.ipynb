{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soLYPcM3luCc"
      },
      "source": [
        "# Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dezur07AluCi"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(['gutenberg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuZUyR8SluCi"
      },
      "outputs": [],
      "source": [
        "ls /root/nltk_data/corpora/gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxrkpAEHluCj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adnJJhDeluCj"
      },
      "source": [
        "# Некоторые функции NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVF-tbBHluCk"
      },
      "source": [
        "## Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4JyGGKUluCk"
      },
      "outputs": [],
      "source": [
        "some_text = \"\"\"We produce about two million dollars for each hour we work.  The\n",
        "fifty hours is one conservative estimate for how long it we take\n",
        "to get any etext selected, entered, proofread, edited, copyright\n",
        "searched and analyzed, the copyright letters written, etc.  This\n",
        "projected audience is one hundred million readers.  If our value\n",
        "per text is nominally estimated at one dollar, then we produce 2\n",
        "million dollars per hour this year we, will have to do four text\n",
        "files per month:  thus upping our productivity from one million.\n",
        "The Goal of Project Gutenberg is to Give Away One Trillion Etext\n",
        "Files by the December 31, 2001.  [10,000 x 100,000,000=Trillion]\n",
        "This is ten thousand titles each to one hundred million readers,\n",
        "which is 10% of the expected number of computer users by the end\n",
        "of the year 2001.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs-rOZfrluCl"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOtp23fuluCm"
      },
      "outputs": [],
      "source": [
        "sentences = nltk.sent_tokenize(some_text)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKPMxxWqluCn"
      },
      "outputs": [],
      "source": [
        "words = [nltk.word_tokenize(s) for s in sentences]\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEbS2XH-luCo"
      },
      "source": [
        "## Лемматизация и стемминг слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLNFAxNOluCp"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzeCUIFtluCq"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IchC_l8fluCq"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI0dodAGluCr"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "snowball_en = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt9n0kqAluCs"
      },
      "outputs": [],
      "source": [
        "word = 'dogs'\n",
        "print(stemmer.stem(word))\n",
        "print(snowball_en.stem(word))\n",
        "print(lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_m7Gk5HluCs"
      },
      "outputs": [],
      "source": [
        "word = 'walked'\n",
        "print(stemmer.stem(word))\n",
        "print(snowball_en.stem(word))\n",
        "print(lemmatizer.lemmatize(word, wordnet.VERB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTHJ29OEluCt"
      },
      "outputs": [],
      "source": [
        "word = 'drove'\n",
        "print(stemmer.stem(word))\n",
        "print(snowball_en.stem(word))\n",
        "print(lemmatizer.lemmatize(word, wordnet.VERB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WWAQjTcluCt"
      },
      "outputs": [],
      "source": [
        "word = 'seen'\n",
        "print(stemmer.stem(word))\n",
        "print(snowball_en.stem(word))\n",
        "print(lemmatizer.lemmatize(word, wordnet.VERB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZOjR_b8luCt"
      },
      "outputs": [],
      "source": [
        "word = 'домами'\n",
        "print(stemmer.stem(word))\n",
        "print(snowball_en.stem(word))\n",
        "print(WordNetLemmatizer().lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGiZjau3luCu"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "snowball_ru = SnowballStemmer('russian')\n",
        "model = spacy.load(\"ru_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EY0i8hvluCu"
      },
      "outputs": [],
      "source": [
        "word = 'собаки'\n",
        "print(snowball_ru.stem(word))\n",
        "for token in model(word):\n",
        "  print(token.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAKG3YYJluCv"
      },
      "outputs": [],
      "source": [
        "word = 'собаками'\n",
        "print(snowball_ru.stem(word))\n",
        "for token in model(word):\n",
        "  print(token.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6MZyrk4luCv"
      },
      "outputs": [],
      "source": [
        "word = 'ходил'\n",
        "print(snowball_ru.stem(word))\n",
        "for token in model(word):\n",
        "  print(token.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXYgYq5MluCv"
      },
      "outputs": [],
      "source": [
        "word = 'прохаживал'\n",
        "print(snowball_ru.stem(word))\n",
        "for token in model(word):\n",
        "  print(token.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'прохаживался'\n",
        "print(snowball_ru.stem(word))\n",
        "for token in model(word):\n",
        "  print(token.lemma_)"
      ],
      "metadata": {
        "id": "3MiRvksK-Oak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OGnyVn9luCw"
      },
      "source": [
        "## Стоп-слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wner_s_hluCw"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1HhBbPCluCw"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in words[0] if not word in stop_words]\n",
        "filtered_words_2 = list(filter(lambda s: s not in stop_words, words[0]))\n",
        "print(words[0])\n",
        "print(filtered_words)\n",
        "print(filtered_words_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaBNOwIpluCw"
      },
      "outputs": [],
      "source": [
        "print(stopwords.raw('russian')[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65FxUxYnluCx"
      },
      "source": [
        "# Мешок слов\n",
        "По сути это мультисет или счетчик, но конкретно используемый как предстваление текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnrbruFiluCx"
      },
      "outputs": [],
      "source": [
        "reviews = [\n",
        "           'This pasta is very tasty and affordable.',\n",
        "           'This pasta is not tasty and is affordable.',\n",
        "           'This pasta is delicious and cheap.',\n",
        "           'Pasta is tasty and pasta tastes good.',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldG2cNt-luCy"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "words = chain(*map(nltk.word_tokenize, reviews))\n",
        "unique_words = set(map(str.lower, words))\n",
        "unique_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "vocabulary = pd.Series(list(unique_words))\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "pH8Fii87F2NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup = pd.Series({v: k for k, v in vocabulary.items()})\n",
        "lookup"
      ],
      "metadata": {
        "id": "62q5r3kdF_tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FdvXsq6luCz"
      },
      "outputs": [],
      "source": [
        "review_words = [nltk.word_tokenize(r) for r in reviews]\n",
        "review_words[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[[w.lower() for w in review] for review in review_words]"
      ],
      "metadata": {
        "id": "HFXaFXvLEOIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[[lookup[w.lower()] for w in review] for review in review_words]\n"
      ],
      "metadata": {
        "id": "9-Z5XTpKEqG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUP3POCLluCz"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "[Counter(lookup[w.lower()] for w in review) for review in review_words]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjD5ufJtluCz"
      },
      "outputs": [],
      "source": [
        "def word_frequencies(words, lookup):\n",
        "  counters = pd.Series(0, index=lookup.values)\n",
        "  for w in words:\n",
        "    counters[lookup[w]] += 1\n",
        "  return counters\n",
        "\n",
        "word_frequencies(review_words[0], lookup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwlI_19KluC0"
      },
      "outputs": [],
      "source": [
        "freqs = pd.DataFrame([word_frequencies(r_w, lookup) for r_w in review_words])\n",
        "freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11NFOVC8luC0"
      },
      "outputs": [],
      "source": [
        "freqs.columns = lookup.index\n",
        "freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9XBh7ToluC0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TScOeYOpluC1"
      },
      "outputs": [],
      "source": [
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKf1sQnjluC1"
      },
      "outputs": [],
      "source": [
        "df = freqs.reindex(sorted(freqs.columns), axis=1).drop(columns='.')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LGSO05oluC2"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhLo8aZoluC2"
      },
      "source": [
        "# N-граммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2VY-WImluC2"
      },
      "outputs": [],
      "source": [
        "review_words[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxAUurWDluC2"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "bigrams = ngrams(review_words[0], 2)\n",
        "trigrams = ngrams(review_words[0], 3)\n",
        "fourgrams = ngrams(review_words[0], 4)\n",
        "\n",
        "bigrams, trigrams, fourgrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh39HqOXluC2"
      },
      "outputs": [],
      "source": [
        "list(bigrams), list(trigrams), list(fourgrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "865LuRYsluC2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "alice_words = nltk.word_tokenize(nltk.corpus.gutenberg.raw(\"carroll-alice.txt\"))\n",
        "ng = ngrams(alice_words, 2)\n",
        "Counter(ng).most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "X = vectorizer.fit_transform([nltk.corpus.gutenberg.raw(\"carroll-alice.txt\")])\n",
        "X"
      ],
      "metadata": {
        "id": "aD7pXOxhLB9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(X.toarray()[0], index=vectorizer.get_feature_names_out()).sort_values(ascending=False).head(20)\n"
      ],
      "metadata": {
        "id": "KfvK0oWLLxab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsDmIhKgluC3"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-4RxWuoluC3"
      },
      "outputs": [],
      "source": [
        "reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t195k3rdluC3"
      },
      "outputs": [],
      "source": [
        "docs = [r_w[:-1] for r_w in review_words]\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NyVd3RAluC4"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyTglJKoluC4"
      },
      "outputs": [],
      "source": [
        "def tf(word, doc):\n",
        "    return doc.count(word)\n",
        "\n",
        "tf('pasta', docs[0]), len(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgI_8mypluC4"
      },
      "outputs": [],
      "source": [
        "def df(word, docs):\n",
        "    return sum(1 for doc in docs if word in doc)\n",
        "\n",
        "df('pasta', docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbC14ySaluC4"
      },
      "outputs": [],
      "source": [
        "def idf(word, docs):\n",
        "    N = len(docs)\n",
        "    return math.log((1 + N) / (1 + df(word, docs))) + 1\n",
        "\n",
        "idf('pasta', docs) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8vG8Iy6luC5"
      },
      "outputs": [],
      "source": [
        "def tf_idf(word, doc, docs):\n",
        "    return tf(word, doc) * idf(word, docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuI8PpwdluC5"
      },
      "outputs": [],
      "source": [
        "[tf_idf(w, docs[0], docs) for w in docs[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg948kNZluC5"
      },
      "outputs": [],
      "source": [
        "v1 = list(zip(docs[0], [tf_idf(w, docs[0], docs) for w in docs[0]]))\n",
        "v1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "norm = math.sqrt(sum(v**2 for _, v in v1))\n",
        "[(k, v/norm) for k, v in v1]"
      ],
      "metadata": {
        "id": "cPQK_H37Pe4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQW7U7wAluC5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "values = tfidf_vectorizer.fit_transform(reviews)\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "pd.DataFrame(values.toarray(), columns = feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwrfOaUhluC6"
      },
      "source": [
        "## Извлечение ключевых слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "See_uPtfluC6"
      },
      "outputs": [],
      "source": [
        "names = nltk.corpus.gutenberg.fileids()\n",
        "names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF9ZHOqLluC6"
      },
      "outputs": [],
      "source": [
        "texts = [nltk.corpus.gutenberg.raw(n) for n in names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1mymrLRluC6"
      },
      "outputs": [],
      "source": [
        "corpus = pd.DataFrame({'Name': names, 'Text': texts})\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZnGJpeOluC7"
      },
      "outputs": [],
      "source": [
        "# corpus['Text'] = corpus['Text'].apply(lambda t: t.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPCAoz5luC7"
      },
      "outputs": [],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVRau_rcluC7"
      },
      "outputs": [],
      "source": [
        "vectorizer=CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(corpus['Text'])\n",
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iezdnoCSluC8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer().fit(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BV_GnOLluC8"
      },
      "outputs": [],
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_names[2000:2010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti5ruQ59luC8"
      },
      "outputs": [],
      "source": [
        "doc = corpus[\"Text\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LBwugJ8luC9"
      },
      "outputs": [],
      "source": [
        "tf_idf_vector=tfidf.transform(vectorizer.transform([doc]))\n",
        "tf_idf_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr_MV2vdluC9"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "from typing import Dict\n",
        "\n",
        "def vector_to_dict(vector: coo_matrix) -> Dict[int, float]:\n",
        "    return {k: v for k, v in zip(vector.col, vector.data)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rlhyVvsluDC"
      },
      "outputs": [],
      "source": [
        "token_scores = vector_to_dict(tf_idf_vector.tocoo())\n",
        "token_scores = pd.DataFrame(token_scores.items(), columns=[\"word_id\", \"score\"])\n",
        "token_scores = token_scores.sort_values(\"score\", ascending=False)\n",
        "token_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XKBYIYaluDD"
      },
      "outputs": [],
      "source": [
        "token_scores['word'] = np.array(feature_names)[token_scores.word_id]\n",
        "token_scores.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGUSbuYluDD"
      },
      "outputs": [],
      "source": [
        "token_scores.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ083g-YluDD"
      },
      "outputs": [],
      "source": [
        "token_scores.head()[['word', 'score']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfcrf4QzluDD"
      },
      "outputs": [],
      "source": [
        "def get_keywords(text, n=10, tfidf=tfidf, vectorizer=vectorizer):\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf.transform(vectorizer.transform([text]))\n",
        "    token_scores = pd.DataFrame(\n",
        "        vector_to_dict(tf_idf_vector.tocoo()).items(),\n",
        "        columns=[\"word_id\", \"score\"]\n",
        "    )\n",
        "    token_scores['word'] = np.array(vectorizer.get_feature_names_out())[token_scores.word_id]\n",
        "    top = token_scores.sort_values(\"score\", ascending=False).head(n)\n",
        "    top.score = np.round(top.score, 3)\n",
        "    return {word: score for word, score in top[[\"word\", \"score\"]].values}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4h1C9bDluDE"
      },
      "outputs": [],
      "source": [
        "keywords = get_keywords(corpus[\"Text\"][7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK0aBHKSluDE"
      },
      "outputs": [],
      "source": [
        "for k in keywords:\n",
        "    print(k, keywords[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4J_-N9OluDE"
      },
      "outputs": [],
      "source": [
        "corpus['Keywords'] = corpus[\"Text\"].map(get_keywords)\n",
        "corpus.Keywords[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI9KJS3rluDF"
      },
      "outputs": [],
      "source": [
        "corpus['kw'] = corpus[\"Keywords\"].map(lambda d: \" \".join(d.keys()))\n",
        "corpus.kw[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxWujN3jluDF"
      },
      "outputs": [],
      "source": [
        "corpus[[\"Name\", \"kw\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvci5F2hluDF"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"max_colwidth\", 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JkCQC_RluDF"
      },
      "outputs": [],
      "source": [
        "corpus[[\"Name\", \"Keywords\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuwZD62OluDF"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"max_colwidth\", 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkruc7wOluDF"
      },
      "source": [
        "## Сходство документов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-n2UKTyluDF"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "pipe = Pipeline([('count', vectorizer), ('idf', tfidf)])\n",
        "tf_idf_vector = pipe.transform(corpus.Text)\n",
        "tf_idf_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKLfPnTFluDG"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance\n",
        "print(distance.euclidean([10, 10], [13, 14]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyyPAFmlluDG"
      },
      "outputs": [],
      "source": [
        "distance.euclidean(tf_idf_vector[7].toarray(), tf_idf_vector[8].toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TqWrHAVluDH"
      },
      "outputs": [],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-m22ZBlluDH"
      },
      "outputs": [],
      "source": [
        "a = corpus[['Name']].reset_index()\n",
        "cross = a.merge(a, how='cross')\n",
        "cross"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV_S1BtLluDH"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "product_ = pd.DataFrame(product(corpus.index, corpus.index), columns=['id1', 'id2'])\n",
        "product_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGHhJhMMluDH"
      },
      "outputs": [],
      "source": [
        "corpus.Name.loc[product_.id1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI90uKf8luDI"
      },
      "outputs": [],
      "source": [
        "product_['Name1'] = corpus.Name.loc[product_.id1].values\n",
        "product_['Name2'] = corpus.Name.loc[product_.id2].values\n",
        "product_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTd8PNoBluDJ"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(id1, id2, tf_idf_vector=tf_idf_vector):\n",
        "    return distance.euclidean(tf_idf_vector[id1].toarray(), tf_idf_vector[id2].toarray())\n",
        "\n",
        "product_['Distance'] = product_.apply(lambda x: euclidean_distance(x.id1, x.id2), axis=1)\n",
        "product_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQfXXK8lluDK"
      },
      "outputs": [],
      "source": [
        "result = product_.sort_values(by=['Distance'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j90aq9HluDL"
      },
      "outputs": [],
      "source": [
        "result[result['Distance'] > 0].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qar13lYCluDL"
      },
      "source": [
        "# Задание\n",
        "1. Извлечь ключевые слова из всех текстов в корпусе, после устранения стоп-слов\n",
        "2. Найти ключевые триграммы для текстов (без устранения стоп-слов)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}